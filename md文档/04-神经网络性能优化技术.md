# 神经网络性能优化技术

`原文链接：https://blog.csdn.net/LOVEmy134611/article/details/131507450`

本节主要描述超参数的调试过程

## 1.数据准备

**下载数据**

```python
from torchvision import datasets
import torch
data_folder='./data/FMNIST' # 下载数据后的存储路径
fmnist=datasets.FashionMNIST(data_floder,download=True,train=True) #是否需要下载到本地，对应数据集为训练数据集
```

**存储对应图像与标签**

```python
tr_images=fmnist.data #存储训练图像
tr_targets=fmnist.targets #存储对应标签
```

**检查张量的维度**

```python
unique_values = tr_targets.unique()
print(f'tr_images & tr_targets:\n\tX - {tr_images.shape}\n\tY - {tr_targets.shape}\n\tY - Unique Values : {unique_values}')
print(f'TASK:\n\t{len(unique_values)} class Classification')
print(f'UNIQUE CLASSES:\n\t{fmnist.classes}') 
```

```python
tr_images & tr_targets:
        X - torch.Size([60000, 28, 28])
        Y - torch.Size([60000])
        Y - Unique Values : tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
TASK:
        10 class Classification
UNIQUE CLASSES:
        ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
```

**尝试绘图**

```python
import matplotlib.pyplot as plt
import numpy as np

R,C=len(tr_targets.unique()),10
fig,ax=plt.subplots(R,C,figsize=(10,10))
for label_class,plot_row in enumerate(ax):
    label_x_row=np.where(tr_target==label_class)[0]
```

*避坑指南:*

*1.enumerate()函数是干什么的？同时获取索引数(默认0开始)与迭代对象，这里以行遍历的ax小型图像，是一个以ax为元素的数组*

*2.np.where()是干嘛的？从数组(即分类结果中)挑选出于对应索引(当前类)一致的行索引*

*3.为什么要加[0]？输出结果为一个元组，获取第一个元素后才能得到对应行索引的数组，np.where函数会默认以元组形式分别输出行列索引，这里没有列索引*

*4.为什么是10\*10？是分别从10个类别中随机选取10个样本进行展示*

```python
	for plot_cell in plot_row:
        plot_cell.grid(False);plot_cell.axis('off')
        ix=np.random.choice(label_x_rows) # 从0到label_x_rows中随机抽取1个元素、默认只取1个元素
        x,y=tr_images[ix],tr_targets[ix]
        plot_cell.imshow(x,cmap='gray')
plt.show()
```

## 2.Pytorch训练神经网络

*相关过程：*

*1.导入相关库*

*2.构建数据集*

*3.DataLoader封装数据集*

*4.构建模型、定义损失函数和优化器*

*5.定义训练与验证的模型*

*6.计算准确率(自定义成本函数)*

*7.mini_bach.对每个epoch进行迭代*

**导入相关库**

```python
# 神经架构相关库
from torch.utils.data import Dataset,DataLoader
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt

# 数据集相关库
from torchvision import datasets

# 分配设备
device="cuda" if torch.cuda.is_avaliable() else "cpu"

# 下载数据
data_folder='./data/FMNIST'
fmnist=datasets.FashionMNIST(data_folder,download=True,train=True)
tr_images=fmnist.data
tr_targets=fmnist.targets
```

**构建数据集**

```python
class FMNISTDataset(Dataset):
    def __init__(self,x,y):
        x=x.float()
        x=x.view(-1,28*28) 
        # 数组重塑(第一维度自动计算，第二维度为28*28=784)，同时新建了引用.相当于把图片的所有像素转换为一维数组，吴恩达的编程中出现过类似过程
        self.x,self.y=x,y
    def __getitem__(self,ix):
        x,y=self.x[ix],self.y[iy]
        return x.to(device),y.to(device)
    def __len__(self):
        return len(self.x)
```

**定义数据获取函数**

```python
def get_data():
    train=FMNISDataset(tr_images,tr_targets)
    trn_dl=DataLoader(train,batch_size=32,shuffle=True)
    # 以32个样本为一个batch进行提取，每次提取打乱顺序(不放回抽取)
    return trn_dl
```

**定义模型(损失函数与优化器)**

```python
from torch.optim import SGD
def get_model():
    model=nn.Sequential(
    	nn.Linear(28*28,1000),
        # 输入尺度28*28，输出一层1000隐藏层
        nn.ReLU(),
        nn.Linear(1000,10)
        # 最终分类为10类
    ).to(device)
    loss_fn=nn.CrossEntropyLoss() # 交叉熵损失
    optimizer=SGD(model.parameters(),lr=1e-2)
    return model,loss_fn,optimizer
```

**训练模型**

```python
def train_batch(x,y,model,optimizer,loss_fn):
    # 开启训练模式
    model.train()
    # 梯度清零
    optimizer.zero_grad()
    prediction=model(x)
    batch_loss=loss_fn(prediction,y)
    # 求导
    batch_loss.backward()
    # batch步进
    optimizer.step()
    return batch_loss.item() # 当前batch的损失值，为标量
    
```

**计算准确率**

```python
# 上下文管理,节省内存
@torch.no_grad()
def accuracy(x,y,model):
    model.eval()
    # 进入验证过程
    prediction=model(x)
    # 获取每个样本中计算数值（不是概率，没有归一化）的最大值
    # max()函数返回最大值与对应索引，-1表示在tensor最后一个维度进行，即10类的维度，因为prediction是(batch_size,10)的维度
    max_values,argmaxs=prediction.max(-1)
    is_correct=argmaxs==y
    # 转换设备，输出常规列表，为什么要两个同时用？
    return is_correct.cpu().numpy().tolist()
    
```

**训练神经网络**

```python
trn_dl=get_data()
model,loss_fn,optimizer=get_model()

losses,accuracies=[],[]
for epoch in range(10):
    # 迭代次数，同样的训练数据训练10次
    print(epoch)
    
    # 每次迭代中的损失与准确率
    epoch_losses,epoch_accuracies=[],[]
    for ix,batch in enumerate(iter(trn_dl)):
        # 每个小样本的迭代过程
        x,y=batch # ix为epoch序号，x、y分别表示每个epoch对应数据
        batch_loss=train_batch(x,y,model,optimizer,loss_fn)
        epoch_losses.append(batch_loss)
    epoch_loss=np.array(epoch_losses).mean()
    
    # 每次迭代中的准确率
    for ix,batch in enumerate(iter(trn_dl)):
        # 每个小样本的迭代过程
        x,y=batch
        is_correct=accuracy(x,y,model)
        epoch_accuracies.append(is_correct)
    epoch_accuracy=np.mean(epoch_accuracies)
    
    # 每个小样本均值后作为每次迭代的结果
    losses.append(epoch_loss)
    accuraies.append(epoch_accuracy)
```

*避坑指南：*

*1.DataLoader中shuffle打乱只进行一次，所以上面两次遍历时，使用数据的结果时完全一样的*

**绘制训练损失与准确率随时间变化**

```python
epoches=np.arange(10)+1

plt.figure(figsize=(20,5))
plt.subplot(121)
plt.title('损失函数随迭代次数变化')
plt.plot(epoches,losses,label='Training Loss')
plt.legend()

plt.subplot(122)
plt.title('准确率随迭代次数变化')
plt.plot(epoches,accuracies,label='Training Accuracy')
plt.gca().set_yticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_yticks()])
# 将竖坐标轴转换为百分形式
plt.legend()

plt.show()
```

### 关于to(device)的说明

`to`函数为将数据或模型进行GPU或CPU搬运的过程，可能需要进行转换的数据或多项包括但不限于以下几种:

* 模型。上述代码中的model模型能够直接to到GPU中。（注意存储模型时将模型转换到cpu上）
* 训练数据。训练或验证数据张量需要配合model进行训练，也需要直接导入GPU配合运算，不然会报错

## 3.超参数优化

**获取数据集**

```python
from torch.utils.data import DataSet,DataLoader
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pypolt as plt
from torchvision import datasets
from torch.optim import SGD

device='cuda' if torch.cuda.is_avaliable() else 'cpu'
data_folder='./data/FMNIST'
fmnist=datasets.FashionMNIST(data_folder,download=True,train=True)
tr_images=fmnist.data
tr_targets=fmnist.targets
```

**获取数据集**

```python
class FMNISTDatasets(Dataset):
    def __init__(self,x,y):
        # 这里除了255，转换为0-1
        # 重要的归一化过程
        x=x.float()/255
        x=x.view(-1,28*28)
        self.x,self.y=x,y
    def __getitem__(self,ix):
        x,y=self.x[ix],self.y[ix]
        # 注意移动数据
        return x.to(device),y.to(device)
    def __len__(self):
        return len(self.x)
```

**定义函数**

```python
def get_data():
    train=FMNISTDataset(tr_images,tr_targets)
    trn_dl=DataLoader(train,batch_size=32,shuffle=True)
    return trn_dl
def get_model():
    model=nn.Sequential(
    	nn.Linear(28*28,1000),
        nn.ReLU(),
        nn.Linear(1000,10)
    ).to(device)
    loss_fn=nn.CrossEntropyLoss()
    optimizer=SGD(model.parameters(),lr=1e-2)
    return model,loss_fn,optimizer
def train_batch(x,y,model,loss_fn,optimizer):
    model.train()
    optimizer.zero_grad()
    prediction=model(x)
    batch_loss=loss_fn(prediction,y)
    batch_loss.backward()
    optimizer.step()
    # 不能加cpu，后续还需要计算梯度
    return batch_loss.item()
@torch.no_grad()
def accuracy(x,y,model):
    model.eval()
    prediction=model(x)
    # 在行上计算最大值，输出为列向量
    max_value,argmax=prediction.max(1)
    is_correct=argmax==y
    # 要转成numpy，所以转到cpu
    return is_correct.cpu().numpy() 
```

**训练模型**

```python
trn_dl=get_data()
model,loss_fn,optimizer=get_model()

losses,accuracies=[],[]
for epoch in range(10):
    # 打印当前迭代轮
    print(epoch)
    # 初始化迭代轮内损失记录数组
    epoch_loss,epoch_accuracies=[],[]
    for ix,batch in enumerate(iter(trn_dl)):
        # 分解数据
        x,y=batch
        batch_loss=train_batch(x,y,model,loss_fn,optimizer)
        # 每个小batch的loss
        epoch_losses.append(batch_loss)
    # 一个迭代轮的损失为所有batch损失的均值
    epoch_loss=np.array(epoch_losses).mean()
    for ix,batch in enumerate(iter(trn_dl)):
        x,y=batch
        # 并未标量，是向量
        is_correct=accuracy(x,y,model)
        epoch_accuracies.extent(is_correct)
    epoch_accuracy=np.mean(epoch_accuracies)
    losses.append(epoch_loss)
    accuracies.append(epoch_accuracy)
    
epochs=np.arange(10)+1
plt.figure(figsize=(20,5))
plt.subplot(121)
plt.title('Loss epochs')
plt.plot(epochs,losses,label='Training Loss')
plt.legend()

plt.subplot(122)
plt.title('Accuracy epochs')
plt.plot(epochs,accuracies,label='Training accuracy')
plt.gca().set_yticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_yticks()])
plt.legend()
plt.show()
```

## 4.修改优化器

**将SGD优化器改为Adam优化器**

```python
from troch.optim import SGD,Adam
def get_model():
    model=nn.Sequantial(
    	nn.Linear(28*28,1000),
        nn.ReLU(),
        nn.Linear(1000,10)
    ).to(device)
    
    loss_fn=nn.CrossEntropyLoss()
    optimizer=Adam(model.parameters(),lr=1e-2)
    return model,loss_fn,optimizer
```

## 5.增加验证集

**获取验证集**

```python
def get_data():
  train=FMNISTDataset(tr_images,tr_targets)
  validation=FMNISTDataset(val_images,val_targets)
  trn_dl=DataLoader(train,batch_size=32,shuffle=True)
  # 不设置batch，将整个数据集视作一个batch
  val_dl=DataLoader(validation,batch_size=len(validation),shuffle=True)
  return trn_dl,val_dl
```

**增加验证过程**

```python
# 迭代10次
for epoch in range(20):
  # 打印迭代次数
  print(epoch)

  #计算损失与准确率
  train_epoch_losses,train_epoch_accuracies=[],[]
  for ix,batch in enumerate(trn_dl):
    x,y=batch
    batch_loss=train_batch(x,y,model,optimizer,loss_fn)
    train_epoch_losses.append(batch_loss)
  train_epoch_loss=np.array(train_epoch_losses).mean()

  #计算迭代中的准确率
  for ix,batch in enumerate(trn_dl):
    x,y=batch
    is_correct=accuracy(x,y,model)
    train_epoch_accuracies.append(is_correct)
  # 本来就是numpy数组
  train_epoch_accuracy=np.mean(train_epoch_accuracies)

  for ix,batch in enumerate(iter(val_dl)):
    x,y=batch
    val_is_correct=accuracy(x,y,model)
    validation_loss=val_loss(x,y,model,loss_fn)
  val_epoch_accuracy=np.mean(val_is_correct)
  train_losses.append(train_epoch_losses)
  train_accuracies.append(train_epoch_accuracies)
  val_losses.append(validation_loss)
  val_accuracies.append(val_epoch_accuracy)
```

## 6.深层神经网络

```python
# 4.定义模型(损失函数与优化器)
def get_model():
    model = nn.Sequential(
        nn.Linear(28 * 28, 1000),
        nn.ReLU(),
        nn.Linear(1000, 512),
        nn.ReLU(),
        nn.Linear(512,10)
    ).to(device)
    loss_fn = nn.CrossEntropyLoss()
    optimizer = Adam(model.parameters(), lr=1e-02)
    return model, loss_fn, optimizer
```

