# 08-学习率优化

**(1)定义输入和输出数据集**

```python
x=[[1],[2],[3],[4]]
y=[[3],[6],[9],[12]]
```

**(2)前向传播**

```python
from copy import deepcopy
import numpy as np
def feed_forward(input,outputs,weights):
    # 此处w与b进行了分开存放，不太合规矩
    out=np.dot(inputs,weights[0])+weights[1]
    mean_squared_error=np.mean(np.square(out-outputs))
    return mean_squared_error
```

**(3)更新参数**

```python
def update_weights(inputs,outputs,weights,lr):
    # 复制原始参数
    original_weights=deepcopy(weights)
    # 记录loss
    org_loss=feed_forward(inputs,outputs,original_weights)
    updated_weights=deepcopy(weights)
    
    # 遍历每个层，这个数据只有一层，没啥用
    for i,layer in enumerate(original_weights):
        for index,weight in np.ndenumerate(layer):
            # 遍历w参数变量
            temp_weights=deepcopy(weights)
            temp_weights[i][index]+=0.0001
       			_loss_plus=feed_forward(inputs,outputs,temp_weights)
            grad=(_loss_plus-org_loss)/(0.0001)
            # 最小化标准函数为减，最大化标准函数为加
            updated_weights[i][index]-=grad*lr
     return updated_weights
```

![image-20240829173133882](E:\DeepLearning\md文档\images\image-20240829173133882.png)

`注:w参数为三维数组，第一维为层，第二维为节点，第三维度为上一层的特征`

`注:每层维度为（当前层节点数，上一层节点数）`

**(4)权重与偏差初始化**

```python
W=[np.array([[0]],dtype=float32),np.array([[0]],dtype=float32)]
```

**(5)记录参数，绘图**

```python
weight_value=[]
for epx in range(1000):
    print("epoche:{0}".format(epx))
    W=update_weights(x,y,W,0.01)
    # 只记录第一个权重的数据
    weight_value.append(W[0][0][0])
```

**(6)绘图**

```python
plt.plot(weight_value)
plt.title("Weight values over increasing epoches")
plt.xlabel("epoches")
plt.ylabel("weight Value")
plt.show()
```

![image-20240829173618098](E:\DeepLearning\md文档\images\image-20240829173618098.png)

**(7)对比学习率**

0.01学习率

![image-20240829180103082](E:\DeepLearning\md文档\images\image-20240829180103082.png)

0.1学习率

![image-20240829180221941](E:\DeepLearning\md文档\images\image-20240829180221941.png)

**总结：需要恰当的学习率才能保证最终的学习过程保持较高的学习优势**

**(8)不同学习率的参数分布**

```python
# 此处需要特别注意model.parameters的参数结构
for ix, par in enumerate(model.parameters()):
    if(ix == 0):
        plt.subplot(141)
        plt.hist(par.cpu().detach().numpy().flatten())
        plt.title('Distribution of weights conencting input to hidden layer')
    elif(ix == 1):
        plt.subplot(142)
        plt.hist(par.cpu().detach().numpy().flatten())
        plt.title('Distribution of biases of hidden layer')
    elif(ix == 2):
        plt.subplot(143)
        plt.hist(par.cpu().detach().numpy().flatten())
        plt.title('Distribution of weights conencting hidden to output layer')
    elif(ix == 3):
        plt.subplot(144)
        plt.hist(par.cpu().detach().numpy().flatten())
        plt.title('Distribution of biases of output layer')

plt.show()
```

**结论:**

* 过高的学习率会导致整体的模型不稳定，参数分布范围过大，无法得到正常结果
* 中等的学习率相对来说更加稳定，但需要在特定情况下进行判断是否为中等范围，需要一定程度的实验才能进行
* 过低的学习率可能需要更多的训练时间，同时最终的训练结果可能导致数据过拟合现象，下章将讲述如何处理数据的过拟合问题