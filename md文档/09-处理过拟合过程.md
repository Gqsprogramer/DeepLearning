# 09-处理过拟合

## Dropout方法

使用nn模块中的nn.Dropout()函数进行处理，放置于线性激活层之前

```python
from torch.optim import SGD,Adam
def get_model():s
    model=nn.Sequential(
        # 以0.5的概率筛选节点
    	nn.Dropout(0.5)
        nn.Linear(28*28,1000),
        nn.ReLu(),
        nn.Dropout(0.5),
        nn.Linear(1000,10)
    ).to(device)
    
    loss_fn=nn.CrossEntropyLoss()
    optimizer=Adam(model.parameters(),lr=1e-3)
    return model,loss_fn,optimizer
```

![image-20240830105047399](E:\DeepLearning\md文档\images\image-20240830105047399.png)

**过拟合结果**

![image-20240830110822250](E:\DeepLearning\md文档\images\image-20240830110822250.png)

`好家伙，验证数据集效果竟然比训练数据集更好`

**l1正则化**

```python
# 增加l1范数
def batch_train(x,y,model,loss_fn,optimizer):
    # 进入训练模式
    model.train()

    optimizer.zero_grad()
    prediction=model(x)

    l1_ragularzation=0
    for param in model.parameters():
        l1_ragularzation+=torch.norm(param,1)

    # 0.0001为超参数
    batch_loss=loss_fn(prediction,y)+0.0001*l1_ragularzation
    batch_loss.backward()
    optimizer.step()

    return batch_loss.item()
```

![image-20240830113208237](E:\DeepLearning\md文档\images\image-20240830113208237.png)

相对而言拟合效果会好一些，但是不是很明显

**l2正则化**

```python
# 增加l2范数
def batch_train(x,y,model,loss_fn,optimizer):
    # 进入训练模式
    model.train()

    optimizer.zero_grad()
    prediction=model(x)

    l2_ragularzation=0
    for param in model.parameters():
        l2_ragularzation+=torch.norm(param,2)

    # 0.01为超参数
    batch_loss=loss_fn(prediction,y)+0.01*l2_ragularzation
    batch_loss.backward()
    optimizer.step()

    return batch_loss.item()
```

![image-20240830172831250](E:\DeepLearning\md文档\images\image-20240830172831250.png)

`可能是偶然性因素，l2范数的效果其实比l1还差一些`

**学习率衰减**

针对前几个`epoch`的效果进行统计，如果效果没有明显提升，就更换对应的学习率

```python
from torch import optim
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=0.5,patience=0,threshold = 0.001,verbose=True,min_lr = 1e-5,threshold_mode = 'abs')
```

- `optimizer`: 这是你正在使用的优化器，例如`torch.optim.SGD`或`torch.optim.Adam`。

- `factor`: 学习率减少的因子。在这个例子中，学习率会在满足条件时减少到当前学习率的50%（即乘以0.5）。

- `patience`: 在减少学习率之前，模型性能可以没有改善的epoch数。在这个例子中，`patience=0`意味着只要性能没有改善，学习率就会立即减少。

- `threshold`: 用于决定性能是否有所改善的阈值。在这个例子中，`threshold=0.001`意味着性能必须改善至少0.001才能被认为是真正的改善。

- `verbose`: 如果设置为`True`，调度器会在每次学习率改变时打印一条消息。**<font color='red'>当前参数已被废除，建议使用get_last_lr()进行打印</font>**

  ```python
  for epoch in range(20):
      ...
      # 更新学习率调度器
      scheduler.step(validation_loss)
  
      # 获取并打印最新的学习率
      last_lr = scheduler.get_last_lr()
      print(f"Epoch {epoch}: Learning Rate = {last_lr}")
  ```

  

- `min_lr`: 学习率的下限。在这个例子中，学习率不会低于`1e-5`。

- `threshold_mode`: 阈值模式，可以是`'rel'`或`'abs'`。`abs`表示后面的epoch比前面的epoch要绝对优势0.0001，`rel`表示后面的epoch要比前面的epoch相对优势0.1%

**注意：学习率衰减过程必须在验证数据集的损失中进行，对应代码如下**

```python
trn_dl,val_dl=get_data()
model,loss_fn,optimizer=get_model()

train_losses,train_accuracies=[],[]
val_losses,val_accuracies=[],[]

# 学习率衰减
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=0.5,patience=0,threshold = 0.001,verbose=True,min_lr = 1e-5,threshold_mode = 'abs')

for epoch in range(20):
    print("epoch:{0}".format(epoch))

    train_epoch_losses,train_epoch_accuracies=[],[]

    # 计算损失
    for ix,batch in enumerate(trn_dl):
        x,y=batch
        batch_loss=batch_train(x,y,model,loss_fn,optimizer)
        train_epoch_losses.append(batch_loss)
    train_epoch_loss=np.mean(train_epoch_losses)

    # 计算准确率
    for ix,batch in enumerate(trn_dl):
        x,y=batch
        batch_accuracy=accuracy(x,y,model)
        train_epoch_accuracies.append(batch_accuracy)
    train_epoch_accuracy=np.mean(train_epoch_accuracies)

    # 计算验证机损失
    for ix,batch in enumerate(val_dl):
        x,y=batch
        val_batch_loss=val_loss(x,y,model,loss_fn)
        val_batch_accuracies=accuracy(x,y,model)
    val_epoch_accuracies=np.mean(val_batch_accuracies)

    scheduler.step(val_batch_loss)

    train_losses.append(train_epoch_loss)
    train_accuracies.append(train_epoch_accuracy)
    val_losses.append(val_batch_loss)
    val_accuracies.append(val_epoch_accuracies)
```

![image-20240830192350789](E:\DeepLearning\md文档\images\image-20240830192350789.png)
